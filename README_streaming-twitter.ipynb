{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Twitter\n",
    "\n",
    "Twitter Stream Processing with Tweepy, Kafka and Spark Structured Streaming.\n",
    "\n",
    "\n",
    "This notebook guides through the development of a basic stream processing application using Twitter data. \n",
    "\n",
    "We will fetch tweets from \n",
    "- Paris\n",
    "- Rome\n",
    "- Berlin\n",
    "- Munich\n",
    "- Vienna\n",
    "- Zurich\n",
    "\n",
    "over the \"Filter realtime tweets\" functionality of the Twitter API. By passing bounding boxes for each of the 6\n",
    "cities to the API, we can restrict the stream of data to all (public) tweets coming specifically from these areas.\n",
    "\n",
    "\n",
    "\n",
    "**Two use cases will be implemented:**\n",
    "- Computing how many tweets per minute are produced in each city and storing these values (on a minute-by-minute basis) in an Azure PostgreSQL database. \n",
    "- Submitting an alert in a Kafka output topic when in one of these cities a certain threshold for the number of tweets per minute is exceeded. \n",
    "\n",
    "\n",
    "**The tools we're gonna make use of are:**\n",
    "- A python script leveraging the Tweepy package for fetching the tweets \n",
    "- Kafka - running on an Azure VM - as input layer for Spark Structured Streaming \n",
    "- Spark Structured Streaming on Databricks Community Edition (Spark 2.3, Scala 2.11)\n",
    "- Azure PostgreSQL DB to persist results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'config/twitter-config.json' file with format: \n",
    "<br> {'access-token': 'XXX',\n",
    " 'access-token-secret': 'XXX',\n",
    " 'api-key': 'XXX',\n",
    " 'api-secret-key': 'XXX'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file 'config/twitter-config.json' necessary\n",
    "with open('config/twitter-config.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Tweepy!\n",
    "\n",
    "Post the most current 5 tweets of my wall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(config['api-key'], config['api-secret-key'])\n",
    "auth.set_access_token(config['access-token'], config['access-token-secret'])\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "\n",
    "for tweet in public_tweets[:5]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Geolocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://gist.github.com/dev-techmoe/ef676cdd03ac47ac503e856282077bf2 for Tweepy's staus object structure.\n",
    "\n",
    "From [Twitter API](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/geo-objects.html): *\"The place object is always present when a Tweet is geo-tagged, while the coordinates object is only present (non-null) when the Tweet is assigned an exact location. If an exact location is provided, the coordinates object will provide a [long, lat] array with the geographical coordinates, and a Twitter Place that corresponds to that location will be assigned.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract posts from the following European cities: \n",
    "- Paris: 2.229335, 48.819298, 2.416376, 48.902579\n",
    "- Rome: 12.358222, 41.777618, 12.632567, 42.016053\n",
    "- Berlin: 13.097992, 52.379442, 13.712705, 52.667389\n",
    "- Munich: 11.373648, 48.063202, 11.699651, 48.240582\n",
    "- Vienna: 16.192143, 48.114450, 16.551614,48.326583\n",
    "- Zurich: 8.460501, 47.324890,  8.616888, 47.432676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test script for Tweepy\n",
    "\n",
    "JSON data should be printed as requiried for the Kafka topic. Of course, you'll need to adjust the config file according to your TwitterDev Account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy \n",
    "from datetime import datetime \n",
    "\n",
    "mapping_dict = {'FR': 'Paris', \n",
    "                'AT': 'Vienna',\n",
    "                'CH': 'Zurich', \n",
    "                'IT': 'Rome'}\n",
    "\n",
    "class StdOutListener(tweepy.StreamListener):\n",
    "    \n",
    "    ''' Handles data received from the stream. '''\n",
    " \n",
    "    def on_status(self, status):\n",
    "        \n",
    "        status_dict = {}\n",
    "        \n",
    "        try: \n",
    "            #known error checking\n",
    "            if status.place == None: \n",
    "                self.on_error('place_none')\n",
    "            elif len(status.place.country_code) != 2: \n",
    "                self.on_error('invalid_cc')\n",
    "\n",
    "            #get text\n",
    "            try: \n",
    "                status_dict['tweet'] = status.extended_tweet['full_text']\n",
    "            except AttributeError: \n",
    "                status_dict['tweet'] = status.text\n",
    "\n",
    "            #get country code\n",
    "            if status.place.country_code == 'DE': \n",
    "\n",
    "                lon = int(status.place.bounding_box.coordinates[0][0][0])\n",
    "\n",
    "                if lon == 13: \n",
    "                    status_dict['city'] = 'Berlin'\n",
    "                elif lon == 11:\n",
    "                    status_dict['city'] = 'Munich'\n",
    "\n",
    "            else: \n",
    "                status_dict['city'] = mapping_dict[status.place.country_code]\n",
    "\n",
    "            #get timestamps \n",
    "            status_dict['timestamp'] = str(status.created_at)\n",
    "\n",
    "            #get hashtags\n",
    "            for hashtag in status.entities['hashtags']:\n",
    "                hashtags = []\n",
    "                hashtags.append(hashtag['text'])\n",
    "\n",
    "            print(json.dumps(status_dict, ensure_ascii = False))\n",
    "            \n",
    "        except: \n",
    "            #unspecified error, skip and continue listening\n",
    "            self.on_error('unspecified')\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        print('Got an error with status code: ' + str(status_code))\n",
    "        return True # To continue listening\n",
    " \n",
    "    def on_timeout(self):\n",
    "        print('Timeout...')\n",
    "        return True # To continue listening\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     \n",
    "    listener = StdOutListener()\n",
    "    auth = tweepy.OAuthHandler(config['api-key'], config['api-secret-key'])\n",
    "    auth.set_access_token(config['access-token'], config['access-token-secret'])\n",
    "\n",
    "    stream = tweepy.Stream(auth, listener)\n",
    "    print('Stream started at {}'.format(datetime.now()))\n",
    "    loc_ls = [2.229335, 48.819298, 2.416376, 48.902579,\n",
    "              12.358222, 41.777618, 12.632567, 42.016053,\n",
    "              13.097992, 52.379442, 13.712705, 52.667389,\n",
    "              11.373648, 48.063202, 11.699651, 48.240582,\n",
    "              16.192143, 48.114450, 16.551614, 48.326583,\n",
    "              8.460501, 47.324890,  8.616888, 47.432676]\n",
    "    stream.filter(locations=loc_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output**: <br> \n",
    "*Stream started at 2018-12-21 12:05:46.941793 <br>\n",
    "{\"tweet\": \"@authentic__one @STban91240 Mais moi je t'apprécie et tu le sais. \\nEt si c'est à cause d'Hazard, dis toi qu'en 2011 c'était le meilleur joueur de la L1. Après tout c'est toujours ça que Neymar n'a pas.\\nSakam.\", \"city\": \"Paris\", \"timestamp\": \"2018-12-21 11:05:44\"} <br>\n",
    "{\"tweet\": \"immagine decisamente accurata https://t.co/KDN7mUtzZF\", \"city\": \"Rome\", \"timestamp\": \"2018-12-21 11:05:45\"} <br>\n",
    "{\"tweet\": \"Outdoor sports - Best fitness equipment @ Kahlenberg https://t.co/HYHx4jbzyk\", \"city\": \"Vienna\", \"timestamp\": \"2018-12-21 11:05:46\"} <br>\n",
    "{\"tweet\": \"@karypant @Mov5Stelle Peggio, ci considera malati da curare. Né più né meno che quello che pensano di omosessuali. Gravissimo! e tutti i docenti che hanno votato 5s muti! Che vergogna \\nLa #scuolapubblica è laica non confessionale\", \"city\": \"Rome\", \"timestamp\": \"2018-12-21 11:05:58\"}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Producer Script\n",
    "\n",
    "If this script runs, we know we get the right data from Twitter. Next step: let's write the Python script 'streaming-twitter.py' (see file) - it'll be used to fetch the tweets and write directly into our Kafka topic on our VM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Kafka on Virtual Machine \n",
    "\n",
    "See: https://kafka.apache.org/quickstart\n",
    "\n",
    "Ubuntu Server 18.04.running on Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1)** Start Ubuntu Server 18.04. VM (in our case running on Azure), enable SSH access <br> \n",
    "**Step 2)** Install necessary packages on VM: \n",
    "- sudo apt install openjdk-8-jre-headless\n",
    "- sudo apt install python3-pip\n",
    "- pip3 install tweepy\n",
    "- pip3 install kafka-python\n",
    "\n",
    "**Step 2)** Edit Kafka config/server_properties, uncomment the following lines\n",
    "- listeners=PLAINTEXT://0.0.0.0:9092 \n",
    "- advertised.listeners=PLAINTEXT://*YOUR_VM_IP_HERE*:9092 \n",
    "\n",
    "**Step 3)** Opening Ports 2181, 2188 and 9092 for inbound connections in the Azure portal (VM > Network) <br>\n",
    "**Step 4)** Loading 'streaming_twitter.py'and 'config/twitter-config.json' into VM (e.g. using nano) <br>\n",
    "**Step 5)** Launching Kafka Zookeeper & Server and cretaing input topic\n",
    "- *Launching Kafka Zookeeper:* bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "- *Launching Kafka Server:* bin/kafka-server-start.sh config/server.properties\n",
    "- *Creating Input Topic:* bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic twitter-input\n",
    "- *Creating Alert Topic:* bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic twitter-alerts\n",
    "\n",
    "You can check if the topics have been created via:\n",
    "- bin/kafka-topics.sh --list --zookeeper localhost:2181\n",
    "\n",
    "You can delete topics via: \n",
    "- bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic twitter-input\n",
    "\n",
    "**Step 6)** Write into topic via 'streaming-twitter.py' file: \n",
    "- python3 streaming-twitter.py\n",
    "\n",
    "**Step 7)** Check if streams are received: \n",
    "- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic twitter-input --from-beginning\n",
    "\n",
    "<br>\n",
    "**Output should be your JSONs tweet by tweet. ** <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup PostgreSQL on Azure \n",
    "\n",
    "Databricks Community Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enable external access to the DB by setting up a corresponding Firewall rule in the Azure portal, see https://docs.microsoft.com/en-us/azure/postgresql/concepts-firewall-rules. <br><br>\n",
    "\n",
    "- Creating a database on PostgreSQL: \n",
    "        CREATE DATABASE twitter;\n",
    "\n",
    "- Creating a table in the database: \n",
    "    \n",
    "        CREATE TABLE tweets (\n",
    "            window_id SERIAL PRIMARY KEY,\n",
    "            window_start TIMESTAMP,\n",
    "            city VARCHAR(10),\n",
    "            tweet_count INTEGER );\n",
    "            \n",
    "- Creating window sequence \n",
    "    \n",
    "        CREATE SEQUENCE w_sequence START 1 INCREMENT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Spark Structured Streaming Operations in Databricks \n",
    "\n",
    "see Scala Files (corresponding to Databricks Notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
